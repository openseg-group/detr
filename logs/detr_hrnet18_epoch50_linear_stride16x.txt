*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 2): env://
| distributed init (rank 3): env://
| distributed init (rank 1): env://
| distributed init (rank 0): env://
| distributed init (rank 7): env://
| distributed init (rank 6): env://
| distributed init (rank 4): env://
| distributed init (rank 5): env://
git:
  sha: 0c08615ce97c4eb077ef8fc19fa99ba5a3ccd630, status: has uncommited changes, branch: master

[32m[12/20 08:12:32 DETR]: [0mNamespace(aux_loss=True, backbone='hrnet18', batch_size=2, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path='//rainbowsecret/dataset/coco', dataset_file='coco', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=50, eval=False, frozen_weights=None, giou_loss_coef=2, gpu=0, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir='outputs/detr_hrnet18_gpu8x_epoch50_linear_stride16x', position_embedding='sine', pre_norm=False, rank=0, remove_difficult=False, resume='auto', seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=8)
Namespace(aux_loss=True, backbone='hrnet18', batch_size=2, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path='//rainbowsecret/dataset/coco', dataset_file='coco', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=50, eval=False, frozen_weights=None, giou_loss_coef=2, gpu=0, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=40, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir='outputs/detr_hrnet18_gpu8x_epoch50_linear_stride16x', position_embedding='sine', pre_norm=False, rank=0, remove_difficult=False, resume='auto', seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=8)
Downloading: "https://opr0mq.dm.files.1drv.com/y4mIoWpP2n-LUohHHANpC0jrOixm1FZgO2OsUtP2DwIozH5RsoYVyv_De5wDgR6XuQmirMV3C0AljLeB-zQXevfLlnQpcNeJlT9Q8LwNYDwh3TsECkMTWXCUn3vDGJWpCxQcQWKONr5VQWO1hLEKPeJbbSZ6tgbWwJHgHF7592HY7ilmGe39o5BhHz7P9QqMYLBts6V7QGoaKrr0PL3wvvR4w" to /home/yuyua/.cache/torch/hub/checkpoints/y4mIoWpP2n-LUohHHANpC0jrOixm1FZgO2OsUtP2DwIozH5RsoYVyv_De5wDgR6XuQmirMV3C0AljLeB-zQXevfLlnQpcNeJlT9Q8LwNYDwh3TsECkMTWXCUn3vDGJWpCxQcQWKONr5VQWO1hLEKPeJbbSZ6tgbWwJHgHF7592HY7ilmGe39o5BhHz7P9QqMYLBts6V7QGoaKrr0PL3wvvR4w
  0%|          | 0.00/81.8M [00:00<?, ?B/s]  1%|          | 784k/81.8M [00:00<00:10, 7.92MB/s]  4%|â–         | 3.39M/81.8M [00:00<00:08, 10.1MB/s]  7%|â–‹         | 6.02M/81.8M [00:00<00:06, 12.1MB/s]  9%|â–‰         | 7.77M/81.8M [00:00<00:06, 11.4MB/s] 11%|â–ˆ         | 8.98M/81.8M [00:00<00:07, 10.9MB/s] 12%|â–ˆâ–        | 10.1M/81.8M [00:00<00:08, 8.65MB/s] 14%|â–ˆâ–Ž        | 11.1M/81.8M [00:01<00:09, 7.65MB/s] 15%|â–ˆâ–        | 11.9M/81.8M [00:01<00:12, 6.08MB/s] 15%|â–ˆâ–Œ        | 12.6M/81.8M [00:01<00:16, 4.29MB/s] 16%|â–ˆâ–Œ        | 13.2M/81.8M [00:01<00:21, 3.38MB/s] 17%|â–ˆâ–‹        | 13.9M/81.8M [00:02<00:24, 2.87MB/s] 18%|â–ˆâ–Š        | 14.8M/81.8M [00:02<00:22, 3.10MB/s] 19%|â–ˆâ–‰        | 15.6M/81.8M [00:02<00:23, 2.91MB/s] 20%|â–ˆâ–ˆ        | 16.5M/81.8M [00:02<00:22, 3.08MB/s] 21%|â–ˆâ–ˆâ–       | 17.4M/81.8M [00:03<00:20, 3.23MB/s] 22%|â–ˆâ–ˆâ–       | 18.3M/81.8M [00:03<00:21, 3.16MB/s] 23%|â–ˆâ–ˆâ–Ž       | 19.1M/81.8M [00:03<00:20, 3.28MB/s] 24%|â–ˆâ–ˆâ–       | 20.0M/81.8M [00:04<00:18, 3.48MB/s] 26%|â–ˆâ–ˆâ–Œ       | 20.9M/81.8M [00:04<00:17, 3.74MB/s] 27%|â–ˆâ–ˆâ–‹       | 21.8M/81.8M [00:04<00:16, 3.88MB/s] 28%|â–ˆâ–ˆâ–Š       | 22.6M/81.8M [00:04<00:15, 4.12MB/s] 29%|â–ˆâ–ˆâ–‰       | 23.5M/81.8M [00:04<00:15, 4.07MB/s] 30%|â–ˆâ–ˆâ–‰       | 24.4M/81.8M [00:05<00:15, 3.86MB/s] 31%|â–ˆâ–ˆâ–ˆ       | 25.3M/81.8M [00:05<00:16, 3.61MB/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 26.1M/81.8M [00:05<00:15, 3.79MB/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27.0M/81.8M [00:05<00:13, 4.11MB/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 27.9M/81.8M [00:06<00:13, 4.30MB/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 28.8M/81.8M [00:06<00:12, 4.53MB/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 29.6M/81.8M [00:06<00:12, 4.44MB/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 30.5M/81.8M [00:06<00:12, 4.23MB/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 31.4M/81.8M [00:06<00:14, 3.76MB/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 32.3M/81.8M [00:07<00:14, 3.66MB/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 33.1M/81.8M [00:07<00:13, 3.65MB/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 34.0M/81.8M [00:07<00:13, 3.81MB/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 34.9M/81.8M [00:07<00:13, 3.58MB/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 35.8M/81.8M [00:08<00:13, 3.64MB/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 36.6M/81.8M [00:08<00:12, 3.93MB/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 37.5M/81.8M [00:08<00:10, 4.31MB/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 38.4M/81.8M [00:08<00:09, 4.71MB/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 39.3M/81.8M [00:08<00:08, 5.04MB/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 40.1M/81.8M [00:09<00:08, 5.42MB/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 41.0M/81.8M [00:09<00:07, 5.58MB/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 41.9M/81.8M [00:09<00:07, 5.83MB/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 42.8M/81.8M [00:09<00:07, 5.49MB/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 43.6M/81.8M [00:09<00:08, 4.90MB/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 44.5M/81.8M [00:09<00:08, 4.78MB/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 45.4M/81.8M [00:10<00:09, 4.21MB/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 46.3M/81.8M [00:10<00:09, 4.08MB/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 47.1M/81.8M [00:10<00:10, 3.31MB/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 48.0M/81.8M [00:10<00:09, 3.88MB/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48.9M/81.8M [00:11<00:08, 4.20MB/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 49.8M/81.8M [00:11<00:08, 3.94MB/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 50.6M/81.8M [00:11<00:08, 3.83MB/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 51.5M/81.8M [00:11<00:07, 4.03MB/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 52.4M/81.8M [00:12<00:07, 4.39MB/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 53.3M/81.8M [00:12<00:06, 4.43MB/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 54.1M/81.8M [00:12<00:06, 4.63MB/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 55.0M/81.8M [00:12<00:05, 5.06MB/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 55.9M/81.8M [00:12<00:05, 5.29MB/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 56.8M/81.8M [00:12<00:04, 5.36MB/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 57.6M/81.8M [00:13<00:04, 5.67MB/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58.5M/81.8M [00:13<00:04, 5.92MB/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 59.4M/81.8M [00:13<00:03, 6.26MB/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 60.3M/81.8M [00:13<00:03, 6.34MB/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 61.1M/81.8M [00:13<00:04, 4.97MB/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 62.0M/81.8M [00:13<00:04, 4.56MB/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 62.9M/81.8M [00:14<00:04, 4.38MB/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 63.8M/81.8M [00:14<00:04, 4.26MB/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 64.6M/81.8M [00:14<00:03, 4.58MB/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 65.5M/81.8M [00:14<00:03, 4.94MB/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 66.4M/81.8M [00:14<00:03, 5.33MB/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 67.3M/81.8M [00:14<00:02, 5.93MB/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 68.1M/81.8M [00:15<00:02, 6.11MB/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 69.0M/81.8M [00:15<00:02, 5.93MB/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 69.9M/81.8M [00:15<00:02, 5.17MB/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 70.8M/81.8M [00:15<00:02, 4.60MB/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 71.6M/81.8M [00:16<00:04, 2.33MB/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 72.5M/81.8M [00:16<00:03, 2.97MB/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 73.4M/81.8M [00:16<00:02, 3.72MB/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 75.1M/81.8M [00:17<00:01, 4.56MB/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 76.9M/81.8M [00:17<00:00, 5.40MB/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 77.8M/81.8M [00:17<00:00, 4.63MB/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 78.6M/81.8M [00:17<00:00, 4.27MB/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 79.5M/81.8M [00:17<00:00, 4.25MB/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 80.4M/81.8M [00:18<00:00, 4.18MB/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 81.3M/81.8M [00:18<00:00, 4.99MB/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81.8M/81.8M [00:18<00:00, 4.69MB/s]
[32m[12/20 08:12:53 DETR]: [0mDETR(
  (transformer): LinearTransformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadLinearAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (compress_k): Linear(in_features=32768, out_features=128, bias=False)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_output_proj): Conv2d(382, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (class_embed): Linear(in_features=256, out_features=92, bias=True)
  (bbox_embed): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (query_embed): Embedding(100, 256)
  (backbone): Joiner(
    (0): Backbone(
      (body): HighResolutionNet(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (relu): ReLU(inplace=True)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
            (downsample): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): FrozenBatchNorm2d()
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): FrozenBatchNorm2d()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): FrozenBatchNorm2d()
            (relu): ReLU(inplace=True)
          )
        )
        (transition1): ModuleList(
          (0): Sequential(
            (0): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): FrozenBatchNorm2d()
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Sequential(
              (0): Conv2d(256, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
              (2): ReLU(inplace=True)
            )
          )
        )
        (stage2): Sequential(
          (0): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
              )
            )
            (relu): ReLU(inplace=True)
          )
        )
        (transition2): ModuleList(
          (0): None
          (1): None
          (2): Sequential(
            (0): Sequential(
              (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
              (2): ReLU(inplace=True)
            )
          )
        )
        (stage3): Sequential(
          (0): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (2): Sequential(
                  (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
                (2): Sequential(
                  (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (2): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): None
              )
            )
            (relu): ReLU(inplace=True)
          )
          (1): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (2): Sequential(
                  (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
                (2): Sequential(
                  (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (2): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): None
              )
            )
            (relu): ReLU(inplace=True)
          )
          (2): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (2): Sequential(
                  (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
                (2): Sequential(
                  (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (2): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): None
              )
            )
            (relu): ReLU(inplace=True)
          )
          (3): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (2): Sequential(
                  (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
                (2): Sequential(
                  (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (2): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): None
              )
            )
            (relu): ReLU(inplace=True)
          )
        )
        (transition3): ModuleList(
          (0): None
          (1): None
          (2): None
          (3): Sequential(
            (0): Sequential(
              (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): FrozenBatchNorm2d()
              (2): ReLU(inplace=True)
            )
          )
        )
        (stage4): Sequential(
          (0): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (3): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (2): Sequential(
                  (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (3): Sequential(
                  (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
                (2): Sequential(
                  (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (3): Sequential(
                  (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (2): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): None
                (3): Sequential(
                  (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (3): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (2): Sequential(
                    (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): Sequential(
                  (0): Sequential(
                    (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (3): None
              )
            )
            (relu): ReLU(inplace=True)
          )
          (1): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (3): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (2): Sequential(
                  (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (3): Sequential(
                  (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
                (2): Sequential(
                  (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (3): Sequential(
                  (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (2): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): None
                (3): Sequential(
                  (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (3): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (2): Sequential(
                    (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): Sequential(
                  (0): Sequential(
                    (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (3): None
              )
            )
            (relu): ReLU(inplace=True)
          )
          (2): HighResolutionModule(
            (branches): ModuleList(
              (0): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (1): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (2): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
              (3): Sequential(
                (0): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (1): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (2): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
                (3): BasicBlock(
                  (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): FrozenBatchNorm2d()
                  (relu): ReLU(inplace=True)
                  (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn2): FrozenBatchNorm2d()
                )
              )
            )
            (fuse_layers): ModuleList(
              (0): ModuleList(
                (0): None
                (1): Sequential(
                  (0): Conv2d(36, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (2): Sequential(
                  (0): Conv2d(72, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (3): Sequential(
                  (0): Conv2d(144, 18, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (1): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): None
                (2): Sequential(
                  (0): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
                (3): Sequential(
                  (0): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (2): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): None
                (3): Sequential(
                  (0): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
                  (1): FrozenBatchNorm2d()
                )
              )
              (3): ModuleList(
                (0): Sequential(
                  (0): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(18, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (2): Sequential(
                    (0): Conv2d(18, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (1): Sequential(
                  (0): Sequential(
                    (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                    (2): ReLU(inplace=True)
                  )
                  (1): Sequential(
                    (0): Conv2d(36, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (2): Sequential(
                  (0): Sequential(
                    (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                    (1): FrozenBatchNorm2d()
                  )
                )
                (3): None
              )
            )
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (1): PositionEmbeddingSine()
  )
  (input_proj): Conv2d(270, 256, kernel_size=(1, 1), stride=(1, 1))
)
[32m[12/20 08:12:53 DETR]: [0mnumber of params: 77572708
loading annotations into memory...
Done (t=64.47s)
creating index...
index created!
loading annotations into memory...
Done (t=4.43s)
creating index...
index created!
[32m[12/20 08:14:03 DETR]: [0mStart training
Epoch: [0]  [   0/7393]  eta: 4:02:24  lr: 0.000100  class_error: 97.99  loss: 68.3849 (68.3849)  loss_bbox: 4.8570 (4.8570)  loss_bbox_0: 4.7124 (4.7124)  loss_bbox_1: 4.7361 (4.7361)  loss_bbox_2: 4.7776 (4.7776)  loss_bbox_3: 4.7954 (4.7954)  loss_bbox_4: 4.8280 (4.8280)  loss_ce: 4.3214 (4.3214)  loss_ce_0: 4.6812 (4.6812)  loss_ce_1: 4.5783 (4.5783)  loss_ce_2: 4.6230 (4.6230)  loss_ce_3: 4.4812 (4.4812)  loss_ce_4: 4.3186 (4.3186)  loss_giou: 2.1175 (2.1175)  loss_giou_0: 2.1132 (2.1132)  loss_giou_1: 2.1101 (2.1101)  loss_giou_2: 2.1109 (2.1109)  loss_giou_3: 2.1110 (2.1110)  loss_giou_4: 2.1120 (2.1120)  cardinality_error_unscaled: 89.1250 (89.1250)  cardinality_error_0_unscaled: 91.7500 (91.7500)  cardinality_error_1_unscaled: 89.8125 (89.8125)  cardinality_error_2_unscaled: 90.5625 (90.5625)  cardinality_error_3_unscaled: 90.5625 (90.5625)  cardinality_error_4_unscaled: 86.1250 (86.1250)  class_error_unscaled: 97.9861 (97.9861)  loss_bbox_unscaled: 0.9714 (0.9714)  loss_bbox_0_unscaled: 0.9425 (0.9425)  loss_bbox_1_unscaled: 0.9472 (0.9472)  loss_bbox_2_unscaled: 0.9555 (0.9555)  loss_bbox_3_unscaled: 0.9591 (0.9591)  loss_bbox_4_unscaled: 0.9656 (0.9656)  loss_ce_unscaled: 4.3214 (4.3214)  loss_ce_0_unscaled: 4.6812 (4.6812)  loss_ce_1_unscaled: 4.5783 (4.5783)  loss_ce_2_unscaled: 4.6230 (4.6230)  loss_ce_3_unscaled: 4.4812 (4.4812)  loss_ce_4_unscaled: 4.3186 (4.3186)  loss_giou_unscaled: 1.0587 (1.0587)  loss_giou_0_unscaled: 1.0566 (1.0566)  loss_giou_1_unscaled: 1.0550 (1.0550)  loss_giou_2_unscaled: 1.0555 (1.0555)  loss_giou_3_unscaled: 1.0555 (1.0555)  loss_giou_4_unscaled: 1.0560 (1.0560)  time: 1.9674  data: 0.4091  max mem: 3411
Epoch: [0]  [  10/7393]  eta: 1:21:20  lr: 0.000100  class_error: 100.00  loss: 53.3065 (54.1106)  loss_bbox: 4.2914 (4.4184)  loss_bbox_0: 4.2939 (4.3125)  loss_bbox_1: 4.3703 (4.3753)  loss_bbox_2: 4.4551 (4.4369)  loss_bbox_3: 4.3403 (4.3888)  loss_bbox_4: 4.3010 (4.3985)  loss_ce: 2.1321 (2.3747)  loss_ce_0: 2.2926 (2.5970)  loss_ce_1: 2.1437 (2.4457)  loss_ce_2: 2.1241 (2.4397)  loss_ce_3: 2.1763 (2.4214)  loss_ce_4: 2.1049 (2.3613)  loss_giou: 2.2563 (2.2388)  loss_giou_0: 2.1258 (2.1498)  loss_giou_1: 2.1486 (2.1622)  loss_giou_2: 2.1744 (2.1742)  loss_giou_3: 2.1862 (2.1904)  loss_giou_4: 2.2406 (2.2251)  cardinality_error_unscaled: 5.8750 (13.5114)  cardinality_error_0_unscaled: 5.8750 (18.0341)  cardinality_error_1_unscaled: 5.8750 (17.2557)  cardinality_error_2_unscaled: 5.8750 (15.7727)  cardinality_error_3_unscaled: 5.8750 (13.5966)  cardinality_error_4_unscaled: 5.8750 (13.2727)  class_error_unscaled: 100.0000 (99.8169)  loss_bbox_unscaled: 0.8583 (0.8837)  loss_bbox_0_unscaled: 0.8588 (0.8625)  loss_bbox_1_unscaled: 0.8741 (0.8751)  loss_bbox_2_unscaled: 0.8910 (0.8874)  loss_bbox_3_unscaled: 0.8681 (0.8778)  loss_bbox_4_unscaled: 0.8602 (0.8797)  loss_ce_unscaled: 2.1321 (2.3747)  loss_ce_0_unscaled: 2.2926 (2.5970)  loss_ce_1_unscaled: 2.1437 (2.4457)  loss_ce_2_unscaled: 2.1241 (2.4397)  loss_ce_3_unscaled: 2.1763 (2.4214)  loss_ce_4_unscaled: 2.1049 (2.3613)  loss_giou_unscaled: 1.1282 (1.1194)  loss_giou_0_unscaled: 1.0629 (1.0749)  loss_giou_1_unscaled: 1.0743 (1.0811)  loss_giou_2_unscaled: 1.0872 (1.0871)  loss_giou_3_unscaled: 1.0931 (1.0952)  loss_giou_4_unscaled: 1.1203 (1.1126)  time: 0.6611  data: 0.0499  max mem: 5512
Epoch: [0]  [  20/7393]  eta: 1:11:07  lr: 0.000100  class_error: 100.00  loss: 49.2218 (49.3787)  loss_bbox: 3.8857 (3.8923)  loss_bbox_0: 3.7846 (3.8000)  loss_bbox_1: 3.7681 (3.8004)  loss_bbox_2: 3.8263 (3.8505)  loss_bbox_3: 3.7283 (3.8102)  loss_bbox_4: 3.7875 (3.8401)  loss_ce: 2.0292 (2.1904)  loss_ce_0: 2.2005 (2.3958)  loss_ce_1: 2.0030 (2.2437)  loss_ce_2: 2.0455 (2.2375)  loss_ce_3: 2.0582 (2.2401)  loss_ce_4: 2.0679 (2.1913)  loss_giou: 2.2511 (2.2091)  loss_giou_0: 2.1029 (2.1117)  loss_giou_1: 2.1238 (2.1199)  loss_giou_2: 2.1504 (2.1314)  loss_giou_3: 2.1491 (2.1357)  loss_giou_4: 2.2058 (2.1785)  cardinality_error_unscaled: 5.4375 (9.8899)  cardinality_error_0_unscaled: 5.1875 (12.2560)  cardinality_error_1_unscaled: 5.4375 (11.8512)  cardinality_error_2_unscaled: 5.4375 (11.0744)  cardinality_error_3_unscaled: 5.4375 (9.9345)  cardinality_error_4_unscaled: 5.4375 (9.7589)  class_error_unscaled: 100.0000 (99.9041)  loss_bbox_unscaled: 0.7771 (0.7785)  loss_bbox_0_unscaled: 0.7569 (0.7600)  loss_bbox_1_unscaled: 0.7536 (0.7601)  loss_bbox_2_unscaled: 0.7653 (0.7701)  loss_bbox_3_unscaled: 0.7457 (0.7620)  loss_bbox_4_unscaled: 0.7575 (0.7680)  loss_ce_unscaled: 2.0292 (2.1904)  loss_ce_0_unscaled: 2.2005 (2.3958)  loss_ce_1_unscaled: 2.0030 (2.2437)  loss_ce_2_unscaled: 2.0455 (2.2375)  loss_ce_3_unscaled: 2.0582 (2.2401)  loss_ce_4_unscaled: 2.0679 (2.1913)  loss_giou_unscaled: 1.1256 (1.1045)  loss_giou_0_unscaled: 1.0515 (1.0558)  loss_giou_1_unscaled: 1.0619 (1.0599)  loss_giou_2_unscaled: 1.0752 (1.0657)  loss_giou_3_unscaled: 1.0745 (1.0678)  loss_giou_4_unscaled: 1.1029 (1.0893)  time: 0.5094  data: 0.0112  max mem: 5512
Epoch: [0]  [  30/7393]  eta: 1:07:45  lr: 0.000100  class_error: 100.00  loss: 41.0307 (45.7864)  loss_bbox: 2.8005 (3.3815)  loss_bbox_0: 2.7498 (3.3239)  loss_bbox_1: 2.6766 (3.2913)  loss_bbox_2: 2.6677 (3.3252)  loss_bbox_3: 2.6208 (3.2878)  loss_bbox_4: 2.7139 (3.3247)  loss_ce: 2.0292 (2.1846)  loss_ce_0: 2.2038 (2.3569)  loss_ce_1: 2.1336 (2.2387)  loss_ce_2: 2.0959 (2.2279)  loss_ce_3: 2.1224 (2.2313)  loss_ce_4: 2.0679 (2.1862)  loss_giou: 2.0359 (2.1220)  loss_giou_0: 1.9595 (2.0476)  loss_giou_1: 1.9302 (2.0451)  loss_giou_2: 1.9421 (2.0552)  loss_giou_3: 1.9415 (2.0623)  loss_giou_4: 1.9922 (2.0941)  cardinality_error_unscaled: 6.1875 (8.7319)  cardinality_error_0_unscaled: 6.3125 (10.4234)  cardinality_error_1_unscaled: 6.3125 (10.1492)  cardinality_error_2_unscaled: 6.3125 (9.6230)  cardinality_error_3_unscaled: 6.3125 (8.8185)  cardinality_error_4_unscaled: 6.0625 (8.6270)  class_error_unscaled: 100.0000 (99.8902)  loss_bbox_unscaled: 0.5601 (0.6763)  loss_bbox_0_unscaled: 0.5500 (0.6648)  loss_bbox_1_unscaled: 0.5353 (0.6583)  loss_bbox_2_unscaled: 0.5335 (0.6650)  loss_bbox_3_unscaled: 0.5242 (0.6576)  loss_bbox_4_unscaled: 0.5428 (0.6649)  loss_ce_unscaled: 2.0292 (2.1846)  loss_ce_0_unscaled: 2.2038 (2.3569)  loss_ce_1_unscaled: 2.1336 (2.2387)  loss_ce_2_unscaled: 2.0959 (2.2279)  loss_ce_3_unscaled: 2.1224 (2.2313)  loss_ce_4_unscaled: 2.0679 (2.1862)  loss_giou_unscaled: 1.0179 (1.0610)  loss_giou_0_unscaled: 0.9798 (1.0238)  loss_giou_1_unscaled: 0.9651 (1.0226)  loss_giou_2_unscaled: 0.9710 (1.0276)  loss_giou_3_unscaled: 0.9707 (1.0312)  loss_giou_4_unscaled: 0.9961 (1.0470)  time: 0.4922  data: 0.0088  max mem: 5569
Epoch: [0]  [  40/7393]  eta: 1:05:30  lr: 0.000100  class_error: 100.00  loss: 34.2863 (42.6218)  loss_bbox: 2.0009 (2.9875)  loss_bbox_0: 2.0052 (2.9433)  loss_bbox_1: 1.9118 (2.9050)  loss_bbox_2: 1.9108 (2.9295)  loss_bbox_3: 1.8996 (2.8985)  loss_bbox_4: 1.9402 (2.9316)  loss_ce: 2.0254 (2.1374)  loss_ce_0: 2.1834 (2.2983)  loss_ce_1: 2.1487 (2.1990)  loss_ce_2: 2.1032 (2.1848)  loss_ce_3: 2.1006 (2.1870)  loss_ce_4: 2.0567 (2.1486)  loss_giou: 1.8050 (2.0238)  loss_giou_0: 1.7798 (1.9614)  loss_giou_1: 1.7288 (1.9533)  loss_giou_2: 1.7398 (1.9624)  loss_giou_3: 1.7468 (1.9744)  loss_giou_4: 1.7486 (1.9960)  cardinality_error_unscaled: 5.9375 (7.9649)  cardinality_error_0_unscaled: 5.9375 (9.3369)  cardinality_error_1_unscaled: 5.9375 (9.1311)  cardinality_error_2_unscaled: 5.9375 (8.7317)  cardinality_error_3_unscaled: 5.9375 (8.1143)  cardinality_error_4_unscaled: 5.9375 (7.9040)  class_error_unscaled: 100.0000 (99.6647)  loss_bbox_unscaled: 0.4002 (0.5975)  loss_bbox_0_unscaled: 0.4010 (0.5887)  loss_bbox_1_unscaled: 0.3824 (0.5810)  loss_bbox_2_unscaled: 0.3822 (0.5859)  loss_bbox_3_unscaled: 0.3799 (0.5797)  loss_bbox_4_unscaled: 0.3880 (0.5863)  loss_ce_unscaled: 2.0254 (2.1374)  loss_ce_0_unscaled: 2.1834 (2.2983)  loss_ce_1_unscaled: 2.1487 (2.1990)  loss_ce_2_unscaled: 2.1032 (2.1848)  loss_ce_3_unscaled: 2.1006 (2.1870)  loss_ce_4_unscaled: 2.0567 (2.1486)  loss_giou_unscaled: 0.9025 (1.0119)  loss_giou_0_unscaled: 0.8899 (0.9807)  loss_giou_1_unscaled: 0.8644 (0.9766)  loss_giou_2_unscaled: 0.8699 (0.9812)  loss_giou_3_unscaled: 0.8734 (0.9872)  loss_giou_4_unscaled: 0.8743 (0.9980)  time: 0.4880  data: 0.0089  max mem: 5569
Epoch: [0]  [  50/7393]  eta: 1:04:25  lr: 0.000100  class_error: 100.00  loss: 32.7174 (40.8371)  loss_bbox: 1.6223 (2.7108)  loss_bbox_0: 1.5934 (2.6701)  loss_bbox_1: 1.5603 (2.6341)  loss_bbox_2: 1.5168 (2.6510)  loss_bbox_3: 1.5514 (2.6269)  loss_bbox_4: 1.6085 (2.6577)  loss_ce: 2.0660 (2.1571)  loss_ce_0: 2.1886 (2.3100)  loss_ce_1: 2.1407 (2.2163)  loss_ce_2: 2.0978 (2.1997)  loss_ce_3: 2.1032 (2.2016)  loss_ce_4: 2.0835 (2.1684)  loss_giou: 1.7759 (1.9786)  loss_giou_0: 1.7174 (1.9241)  loss_giou_1: 1.6994 (1.9189)  loss_giou_2: 1.6868 (1.9258)  loss_giou_3: 1.7080 (1.9344)  loss_giou_4: 1.7264 (1.9519)  cardinality_error_unscaled: 6.1875 (7.9681)  cardinality_error_0_unscaled: 6.8750 (9.1250)  cardinality_error_1_unscaled: 6.8750 (8.9596)  cardinality_error_2_unscaled: 6.4375 (8.6029)  cardinality_error_3_unscaled: 6.6250 (8.0490)  cardinality_error_4_unscaled: 6.2500 (7.8603)  class_error_unscaled: 100.0000 (99.6927)  loss_bbox_unscaled: 0.3245 (0.5422)  loss_bbox_0_unscaled: 0.3187 (0.5340)  loss_bbox_1_unscaled: 0.3121 (0.5268)  loss_bbox_2_unscaled: 0.3034 (0.5302)  loss_bbox_3_unscaled: 0.3103 (0.5254)  loss_bbox_4_unscaled: 0.3217 (0.5315)  loss_ce_unscaled: 2.0660 (2.1571)  loss_ce_0_unscaled: 2.1886 (2.3100)  loss_ce_1_unscaled: 2.1407 (2.2163)  loss_ce_2_unscaled: 2.0978 (2.1997)  loss_ce_3_unscaled: 2.1032 (2.2016)  loss_ce_4_unscaled: 2.0835 (2.1684)  loss_giou_unscaled: 0.8880 (0.9893)  loss_giou_0_unscaled: 0.8587 (0.9621)  loss_giou_1_unscaled: 0.8497 (0.9594)  loss_giou_2_unscaled: 0.8434 (0.9629)  loss_giou_3_unscaled: 0.8540 (0.9672)  loss_giou_4_unscaled: 0.8632 (0.9760)  time: 0.4866  data: 0.0087  max mem: 5569
Epoch: [0]  [  60/7393]  eta: 1:03:19  lr: 0.000100  class_error: 100.00  loss: 32.6629 (39.4278)  loss_bbox: 1.5473 (2.5281)  loss_bbox_0: 1.4992 (2.4762)  loss_bbox_1: 1.4687 (2.4424)  loss_bbox_2: 1.4833 (2.4597)  loss_bbox_3: 1.4958 (2.4449)  loss_bbox_4: 1.5301 (2.4725)  loss_ce: 2.1256 (2.1538)  loss_ce_0: 2.1886 (2.2903)  loss_ce_1: 2.1467 (2.2048)  loss_ce_2: 2.1276 (2.1901)  loss_ce_3: 2.1329 (2.1916)  loss_ce_4: 2.1443 (2.1623)  loss_giou: 1.7919 (1.9423)  loss_giou_0: 1.7174 (1.8874)  loss_giou_1: 1.7467 (1.8839)  loss_giou_2: 1.7513 (1.8907)  loss_giou_3: 1.7363 (1.8961)  loss_giou_4: 1.7357 (1.9108)  cardinality_error_unscaled: 6.7500 (7.7141)  cardinality_error_0_unscaled: 6.9375 (8.6916)  cardinality_error_1_unscaled: 6.9375 (8.5092)  cardinality_error_2_unscaled: 6.3125 (8.1465)  cardinality_error_3_unscaled: 6.4375 (7.6332)  cardinality_error_4_unscaled: 6.2500 (7.5717)  class_error_unscaled: 100.0000 (99.7089)  loss_bbox_unscaled: 0.3095 (0.5056)  loss_bbox_0_unscaled: 0.2998 (0.4952)  loss_bbox_1_unscaled: 0.2937 (0.4885)  loss_bbox_2_unscaled: 0.2967 (0.4919)  loss_bbox_3_unscaled: 0.2992 (0.4890)  loss_bbox_4_unscaled: 0.3060 (0.4945)  loss_ce_unscaled: 2.1256 (2.1538)  loss_ce_0_unscaled: 2.1886 (2.2903)  loss_ce_1_unscaled: 2.1467 (2.2048)  loss_ce_2_unscaled: 2.1276 (2.1901)  loss_ce_3_unscaled: 2.1329 (2.1916)  loss_ce_4_unscaled: 2.1443 (2.1623)  loss_giou_unscaled: 0.8960 (0.9712)  loss_giou_0_unscaled: 0.8587 (0.9437)  loss_giou_1_unscaled: 0.8733 (0.9419)  loss_giou_2_unscaled: 0.8757 (0.9454)  loss_giou_3_unscaled: 0.8681 (0.9481)  loss_giou_4_unscaled: 0.8679 (0.9554)  time: 0.4846  data: 0.0087  max mem: 5569
Epoch: [0]  [  70/7393]  eta: 1:02:44  lr: 0.000100  class_error: 99.38  loss: 32.2360 (38.5059)  loss_bbox: 1.5473 (2.3864)  loss_bbox_0: 1.4481 (2.3274)  loss_bbox_1: 1.3996 (2.2950)  loss_bbox_2: 1.4767 (2.3182)  loss_bbox_3: 1.4943 (2.3072)  loss_bbox_4: 1.4560 (2.3297)  loss_ce: 2.1256 (2.1605)  loss_ce_0: 2.1881 (2.2897)  loss_ce_1: 2.1191 (2.2038)  loss_ce_2: 2.1200 (2.1931)  loss_ce_3: 2.1085 (2.1935)  loss_ce_4: 2.0924 (2.1651)  loss_giou: 1.7763 (1.9241)  loss_giou_0: 1.7722 (1.8740)  loss_giou_1: 1.7556 (1.8742)  loss_giou_2: 1.7686 (1.8782)  loss_giou_3: 1.7432 (1.8860)  loss_giou_4: 1.7483 (1.8997)  cardinality_error_unscaled: 6.6250 (7.6690)  cardinality_error_0_unscaled: 6.6875 (8.5141)  cardinality_error_1_unscaled: 6.2500 (8.3107)  cardinality_error_2_unscaled: 6.0000 (8.0141)  cardinality_error_3_unscaled: 6.4375 (7.5977)  cardinality_error_4_unscaled: 6.6250 (7.5563)  class_error_unscaled: 100.0000 (99.7411)  loss_bbox_unscaled: 0.3095 (0.4773)  loss_bbox_0_unscaled: 0.2896 (0.4655)  loss_bbox_1_unscaled: 0.2799 (0.4590)  loss_bbox_2_unscaled: 0.2953 (0.4636)  loss_bbox_3_unscaled: 0.2989 (0.4614)  loss_bbox_4_unscaled: 0.2912 (0.4659)  loss_ce_unscaled: 2.1256 (2.1605)  loss_ce_0_unscaled: 2.1881 (2.2897)  loss_ce_1_unscaled: 2.1191 (2.2038)  loss_ce_2_unscaled: 2.1200 (2.1931)  loss_ce_3_unscaled: 2.1085 (2.1935)  loss_ce_4_unscaled: 2.0924 (2.1651)  loss_giou_unscaled: 0.8881 (0.9621)  loss_giou_0_unscaled: 0.8861 (0.9370)  loss_giou_1_unscaled: 0.8778 (0.9371)  loss_giou_2_unscaled: 0.8843 (0.9391)  loss_giou_3_unscaled: 0.8716 (0.9430)  loss_giou_4_unscaled: 0.8741 (0.9498)  time: 0.4828  data: 0.0087  max mem: 5569
Epoch: [0]  [  80/7393]  eta: 1:02:20  lr: 0.000100  class_error: 92.60  loss: 32.9295 (37.7526)  loss_bbox: 1.4497 (2.2833)  loss_bbox_0: 1.4576 (2.2297)  loss_bbox_1: 1.4023 (2.1979)  loss_bbox_2: 1.4711 (2.2210)  loss_bbox_3: 1.4336 (2.2099)  loss_bbox_4: 1.4260 (2.2295)  loss_ce: 2.1027 (2.1424)  loss_ce_0: 2.1963 (2.2754)  loss_ce_1: 2.1113 (2.1866)  loss_ce_2: 2.1161 (2.1772)  loss_ce_3: 2.0796 (2.1744)  loss_ce_4: 2.0701 (2.1472)  loss_giou: 1.8055 (1.9121)  loss_giou_0: 1.7870 (1.8615)  loss_giou_1: 1.8072 (1.8643)  loss_giou_2: 1.8054 (1.8689)  loss_giou_3: 1.8315 (1.8794)  loss_giou_4: 1.8342 (1.8919)  cardinality_error_unscaled: 7.1250 (7.5471)  cardinality_error_0_unscaled: 7.2500 (8.3241)  cardinality_error_1_unscaled: 6.8125 (8.1003)  cardinality_error_2_unscaled: 6.7500 (7.8194)  cardinality_error_3_unscaled: 7.1875 (7.5231)  cardinality_error_4_unscaled: 7.2500 (7.4853)  class_error_unscaled: 100.0000 (99.5885)  loss_bbox_unscaled: 0.2899 (0.4567)  loss_bbox_0_unscaled: 0.2915 (0.4459)  loss_bbox_1_unscaled: 0.2805 (0.4396)  loss_bbox_2_unscaled: 0.2942 (0.4442)  loss_bbox_3_unscaled: 0.2867 (0.4420)  loss_bbox_4_unscaled: 0.2852 (0.4459)  loss_ce_unscaled: 2.1027 (2.1424)  loss_ce_0_unscaled: 2.1963 (2.2754)  loss_ce_1_unscaled: 2.1113 (2.1866)  loss_ce_2_unscaled: 2.1161 (2.1772)  loss_ce_3_unscaled: 2.0796 (2.1744)  loss_ce_4_unscaled: 2.0701 (2.1472)  loss_giou_unscaled: 0.9027 (0.9561)  loss_giou_0_unscaled: 0.8935 (0.9308)  loss_giou_1_unscaled: 0.9036 (0.9322)  loss_giou_2_unscaled: 0.9027 (0.9344)  loss_giou_3_unscaled: 0.9157 (0.9397)  loss_giou_4_unscaled: 0.9171 (0.9460)  time: 0.4912  data: 0.0085  max mem: 5569
Epoch: [0]  [  90/7393]  eta: 1:02:01  lr: 0.000100  class_error: 100.00  loss: 31.5742 (37.1254)  loss_bbox: 1.4442 (2.1941)  loss_bbox_0: 1.4669 (2.1466)  loss_bbox_1: 1.4688 (2.1190)  loss_bbox_2: 1.4464 (2.1381)  loss_bbox_3: 1.4166 (2.1290)  loss_bbox_4: 1.4317 (2.1450)  loss_ce: 2.0617 (2.1389)  loss_ce_0: 2.1504 (2.2656)  loss_ce_1: 2.0693 (2.1814)  loss_ce_2: 2.0581 (2.1721)  loss_ce_3: 2.0918 (2.1727)  loss_ce_4: 2.0660 (2.1444)  loss_giou: 1.7477 (1.8911)  loss_giou_0: 1.7233 (1.8480)  loss_giou_1: 1.7383 (1.8492)  loss_giou_2: 1.7236 (1.8531)  loss_giou_3: 1.7769 (1.8633)  loss_giou_4: 1.7502 (1.8739)  cardinality_error_unscaled: 7.1250 (7.4368)  cardinality_error_0_unscaled: 7.1875 (8.1957)  cardinality_error_1_unscaled: 6.8125 (7.9643)  cardinality_error_2_unscaled: 6.7500 (7.6305)  cardinality_error_3_unscaled: 7.2500 (7.4217)  cardinality_error_4_unscaled: 7.1875 (7.4210)  class_error_unscaled: 100.0000 (99.5785)  loss_bbox_unscaled: 0.2888 (0.4388)  loss_bbox_0_unscaled: 0.2934 (0.4293)  loss_bbox_1_unscaled: 0.2938 (0.4238)  loss_bbox_2_unscaled: 0.2893 (0.4276)  loss_bbox_3_unscaled: 0.2833 (0.4258)  loss_bbox_4_unscaled: 0.2863 (0.4290)  loss_ce_unscaled: 2.0617 (2.1389)  loss_ce_0_unscaled: 2.1504 (2.2656)  loss_ce_1_unscaled: 2.0693 (2.1814)  loss_ce_2_unscaled: 2.0581 (2.1721)  loss_ce_3_unscaled: 2.0918 (2.1727)  loss_ce_4_unscaled: 2.0660 (2.1444)  loss_giou_unscaled: 0.8738 (0.9455)  loss_giou_0_unscaled: 0.8616 (0.9240)  loss_giou_1_unscaled: 0.8691 (0.9246)  loss_giou_2_unscaled: 0.8618 (0.9266)  loss_giou_3_unscaled: 0.8885 (0.9317)  loss_giou_4_unscaled: 0.8751 (0.9369)  time: 0.4936  data: 0.0086  max mem: 6545
Epoch: [0]  [ 100/7393]  eta: 1:01:39  lr: 0.000100  class_error: 100.00  loss: 31.3517 (36.5345)  loss_bbox: 1.4091 (2.1150)  loss_bbox_0: 1.3939 (2.0689)  loss_bbox_1: 1.4178 (2.0454)  loss_bbox_2: 1.3891 (2.0667)  loss_bbox_3: 1.4199 (2.0607)  loss_bbox_4: 1.4317 (2.0734)  loss_ce: 2.0453 (2.1295)  loss_ce_0: 2.1284 (2.2495)  loss_ce_1: 2.0652 (2.1697)  loss_ce_2: 2.0432 (2.1590)  loss_ce_3: 2.0918 (2.1625)  loss_ce_4: 2.0660 (2.1357)  loss_giou: 1.6985 (1.8767)  loss_giou_0: 1.7278 (1.8364)  loss_giou_1: 1.7306 (1.8380)  loss_giou_2: 1.7133 (1.8400)  loss_giou_3: 1.7085 (1.8480)  loss_giou_4: 1.7052 (1.8593)  cardinality_error_unscaled: 6.8125 (7.3459)  cardinality_error_0_unscaled: 6.8125 (8.0328)  cardinality_error_1_unscaled: 6.6875 (7.8218)  cardinality_error_2_unscaled: 6.6875 (7.5099)  cardinality_error_3_unscaled: 6.8125 (7.3317)  cardinality_error_4_unscaled: 6.8125 (7.3348)  class_error_unscaled: 100.0000 (99.6202)  loss_bbox_unscaled: 0.2818 (0.4230)  loss_bbox_0_unscaled: 0.2788 (0.4138)  loss_bbox_1_unscaled: 0.2836 (0.4091)  loss_bbox_2_unscaled: 0.2778 (0.4133)  loss_bbox_3_unscaled: 0.2840 (0.4121)  loss_bbox_4_unscaled: 0.2863 (0.4147)  loss_ce_unscaled: 2.0453 (2.1295)  loss_ce_0_unscaled: 2.1284 (2.2495)  loss_ce_1_unscaled: 2.0652 (2.1697)  loss_ce_2_unscaled: 2.0432 (2.1590)  loss_ce_3_unscaled: 2.0918 (2.1625)  loss_ce_4_unscaled: 2.0660 (2.1357)  loss_giou_unscaled: 0.8492 (0.9383)  loss_giou_0_unscaled: 0.8639 (0.9182)  loss_giou_1_unscaled: 0.8653 (0.9190)  loss_giou_2_unscaled: 0.8566 (0.9200)  loss_giou_3_unscaled: 0.8543 (0.9240)  loss_giou_4_unscaled: 0.8526 (0.9296)  time: 0.4899  data: 0.0089  max mem: 6545
Epoch: [0]  [ 110/7393]  eta: 1:01:23  lr: 0.000100  class_error: 100.00  loss: 30.3365 (35.9912)  loss_bbox: 1.3877 (2.0568)  loss_bbox_0: 1.3780 (2.0107)  loss_bbox_1: 1.4144 (1.9903)  loss_bbox_2: 1.4259 (2.0117)  loss_bbox_3: 1.4355 (2.0070)  loss_bbox_4: 1.4102 (2.0178)  loss_ce: 1.9615 (2.1114)  loss_ce_0: 2.0453 (2.2249)  loss_ce_1: 1.9661 (2.1494)  loss_ce_2: 1.9713 (2.1390)  loss_ce_3: 2.0065 (2.1430)  loss_ce_4: 1.9656 (2.1192)  loss_giou: 1.6880 (1.8597)  loss_giou_0: 1.6673 (1.8218)  loss_giou_1: 1.6923 (1.8236)  loss_giou_2: 1.7073 (1.8267)  loss_giou_3: 1.7033 (1.8342)  loss_giou_4: 1.6923 (1.8442)  cardinality_error_unscaled: 6.4375 (7.2393)  cardinality_error_0_unscaled: 6.4375 (7.8682)  cardinality_error_1_unscaled: 6.4375 (7.6655)  cardinality_error_2_unscaled: 6.4375 (7.3846)  cardinality_error_3_unscaled: 6.4375 (7.2303)  cardinality_error_4_unscaled: 6.4375 (7.2314)  class_error_unscaled: 100.0000 (99.6544)  loss_bbox_unscaled: 0.2775 (0.4114)  loss_bbox_0_unscaled: 0.2756 (0.4021)  loss_bbox_1_unscaled: 0.2829 (0.3981)  loss_bbox_2_unscaled: 0.2852 (0.4023)  loss_bbox_3_unscaled: 0.2871 (0.4014)  loss_bbox_4_unscaled: 0.2820 (0.4036)  loss_ce_unscaled: 1.9615 (2.1114)  loss_ce_0_unscaled: 2.0453 (2.2249)  loss_ce_1_unscaled: 1.9661 (2.1494)  loss_ce_2_unscaled: 1.9713 (2.1390)  loss_ce_3_unscaled: 2.0065 (2.1430)  loss_ce_4_unscaled: 1.9656 (2.1192)  loss_giou_unscaled: 0.8440 (0.9299)  loss_giou_0_unscaled: 0.8336 (0.9109)  loss_giou_1_unscaled: 0.8461 (0.9118)  loss_giou_2_unscaled: 0.8537 (0.9133)  loss_giou_3_unscaled: 0.8517 (0.9171)  loss_giou_4_unscaled: 0.8462 (0.9221)  time: 0.4884  data: 0.0087  max mem: 6545
Epoch: [0]  [ 120/7393]  eta: 1:00:57  lr: 0.000100  class_error: 100.00  loss: 30.3365 (35.5786)  loss_bbox: 1.4697 (2.0103)  loss_bbox_0: 1.3929 (1.9662)  loss_bbox_1: 1.4144 (1.9458)  loss_bbox_2: 1.4321 (1.9675)  loss_bbox_3: 1.4606 (1.9607)  loss_bbox_4: 1.4102 (1.9711)  loss_ce: 1.9066 (2.1054)  loss_ce_0: 1.9262 (2.2086)  loss_ce_1: 1.9177 (2.1381)  loss_ce_2: 1.9186 (2.1291)  loss_ce_3: 1.9489 (2.1346)  loss_ce_4: 1.9317 (2.1112)  loss_giou: 1.6794 (1.8429)  loss_giou_0: 1.6651 (1.8123)  loss_giou_1: 1.6891 (1.8130)  loss_giou_2: 1.6982 (1.8138)  loss_giou_3: 1.6946 (1.8190)  loss_giou_4: 1.6923 (1.8290)  cardinality_error_unscaled: 5.9375 (7.1658)  cardinality_error_0_unscaled: 6.2500 (7.7650)  cardinality_error_1_unscaled: 6.1875 (7.5770)  cardinality_error_2_unscaled: 6.2500 (7.3197)  cardinality_error_3_unscaled: 6.0000 (7.1699)  cardinality_error_4_unscaled: 6.2500 (7.1725)  class_error_unscaled: 100.0000 (99.6418)  loss_bbox_unscaled: 0.2939 (0.4021)  loss_bbox_0_unscaled: 0.2786 (0.3932)  loss_bbox_1_unscaled: 0.2829 (0.3892)  loss_bbox_2_unscaled: 0.2864 (0.3935)  loss_bbox_3_unscaled: 0.2921 (0.3921)  loss_bbox_4_unscaled: 0.2820 (0.3942)  loss_ce_unscaled: 1.9066 (2.1054)  loss_ce_0_unscaled: 1.9262 (2.2086)  loss_ce_1_unscaled: 1.9177 (2.1381)  loss_ce_2_unscaled: 1.9186 (2.1291)  loss_ce_3_unscaled: 1.9489 (2.1346)  loss_ce_4_unscaled: 1.9317 (2.1112)  loss_giou_unscaled: 0.8397 (0.9214)  loss_giou_0_unscaled: 0.8326 (0.9062)  loss_giou_1_unscaled: 0.8446 (0.9065)  loss_giou_2_unscaled: 0.8491 (0.9069)  loss_giou_3_unscaled: 0.8473 (0.9095)  loss_giou_4_unscaled: 0.8462 (0.9145)  time: 0.4806  data: 0.0086  max mem: 6545
Epoch: [0]  [ 130/7393]  eta: 1:00:41  lr: 0.000100  class_error: 100.00  loss: 31.0244 (35.2650)  loss_bbox: 1.4263 (1.9645)  loss_bbox_0: 1.3866 (1.9211)  loss_bbox_1: 1.3775 (1.9044)  loss_bbox_2: 1.4282 (1.9289)  loss_bbox_3: 1.4113 (1.9204)  loss_bbox_4: 1.4416 (1.9315)  loss_ce: 2.0134 (2.1072)  loss_ce_0: 2.0595 (2.2044)  loss_ce_1: 2.0390 (2.1382)  loss_ce_2: 2.0441 (2.1309)  loss_ce_3: 2.0722 (2.1380)  loss_ce_4: 2.0798 (2.1142)  loss_giou: 1.6769 (1.8310)  loss_giou_0: 1.6810 (1.8025)  loss_giou_1: 1.6501 (1.8023)  loss_giou_2: 1.6542 (1.8018)  loss_giou_3: 1.6431 (1.8081)  loss_giou_4: 1.6472 (1.8158)  cardinality_error_unscaled: 6.6875 (7.2104)  cardinality_error_0_unscaled: 6.6875 (7.7667)  cardinality_error_1_unscaled: 6.5625 (7.5868)  cardinality_error_2_unscaled: 6.6875 (7.3540)  cardinality_error_3_unscaled: 6.5000 (7.2114)  cardinality_error_4_unscaled: 6.6250 (7.2190)  class_error_unscaled: 100.0000 (99.6692)  loss_bbox_unscaled: 0.2853 (0.3929)  loss_bbox_0_unscaled: 0.2773 (0.3842)  loss_bbox_1_unscaled: 0.2755 (0.3809)  loss_bbox_2_unscaled: 0.2856 (0.3858)  loss_bbox_3_unscaled: 0.2823 (0.3841)  loss_bbox_4_unscaled: 0.2883 (0.3863)  loss_ce_unscaled: 2.0134 (2.1072)  loss_ce_0_unscaled: 2.0595 (2.2044)  loss_ce_1_unscaled: 2.0390 (2.1382)  loss_ce_2_unscaled: 2.0441 (2.1309)  loss_ce_3_unscaled: 2.0722 (2.1380)  loss_ce_4_unscaled: 2.0798 (2.1142)  loss_giou_unscaled: 0.8384 (0.9155)  loss_giou_0_unscaled: 0.8405 (0.9012)  loss_giou_1_unscaled: 0.8251 (0.9011)  loss_giou_2_unscaled: 0.8271 (0.9009)  loss_giou_3_unscaled: 0.8216 (0.9040)  loss_giou_4_unscaled: 0.8236 (0.9079)  time: 0.4773  data: 0.0088  max mem: 6545
Epoch: [0]  [ 140/7393]  eta: 1:00:27  lr: 0.000100  class_error: 100.00  loss: 30.8709 (34.9151)  loss_bbox: 1.3739 (1.9291)  loss_bbox_0: 1.3926 (1.8882)  loss_bbox_1: 1.3651 (1.8694)  loss_bbox_2: 1.4127 (1.8943)  loss_bbox_3: 1.4113 (1.8881)  loss_bbox_4: 1.4186 (1.8977)  loss_ce: 2.0134 (2.0954)  loss_ce_0: 2.0285 (2.1882)  loss_ce_1: 1.9891 (2.1239)  loss_ce_2: 2.0355 (2.1187)  loss_ce_3: 2.0469 (2.1264)  loss_ce_4: 2.0415 (2.1019)  loss_giou: 1.6608 (1.8181)  loss_giou_0: 1.6464 (1.7931)  loss_giou_1: 1.6420 (1.7923)  loss_giou_2: 1.5987 (1.7899)  loss_giou_3: 1.6322 (1.7968)  loss_giou_4: 1.6196 (1.8036)  cardinality_error_unscaled: 6.6875 (7.1520)  cardinality_error_0_unscaled: 6.6875 (7.6724)  cardinality_error_1_unscaled: 6.5625 (7.5053)  cardinality_error_2_unscaled: 6.6875 (7.2877)  cardinality_error_3_unscaled: 6.6875 (7.1489)  cardinality_error_4_unscaled: 6.6250 (7.1627)  class_error_unscaled: 100.0000 (99.6779)  loss_bbox_unscaled: 0.2748 (0.3858)  loss_bbox_0_unscaled: 0.2785 (0.3776)  loss_bbox_1_unscaled: 0.2730 (0.3739)  loss_bbox_2_unscaled: 0.2825 (0.3789)  loss_bbox_3_unscaled: 0.2823 (0.3776)  loss_bbox_4_unscaled: 0.2837 (0.3795)  loss_ce_unscaled: 2.0134 (2.0954)  loss_ce_0_unscaled: 2.0285 (2.1882)  loss_ce_1_unscaled: 1.9891 (2.1239)  loss_ce_2_unscaled: 2.0355 (2.1187)  loss_ce_3_unscaled: 2.0469 (2.1264)  loss_ce_4_unscaled: 2.0415 (2.1019)  loss_giou_unscaled: 0.8304 (0.9090)  loss_giou_0_unscaled: 0.8232 (0.8966)  loss_giou_1_unscaled: 0.8210 (0.8961)  loss_giou_2_unscaled: 0.7994 (0.8950)  loss_giou_3_unscaled: 0.8161 (0.8984)  loss_giou_4_unscaled: 0.8098 (0.9018)  time: 0.4840  data: 0.0089  max mem: 7067
Epoch: [0]  [ 150/7393]  eta: 1:00:16  lr: 0.000100  class_error: 100.00  loss: 31.4359 (34.6317)  loss_bbox: 1.3915 (1.8973)  loss_bbox_0: 1.4444 (1.8587)  loss_bbox_1: 1.3968 (1.8402)  loss_bbox_2: 1.4521 (1.8696)  loss_bbox_3: 1.4284 (1.8576)  loss_bbox_4: 1.4209 (1.8672)  loss_ce: 1.9315 (2.0846)  loss_ce_0: 1.9189 (2.1740)  loss_ce_1: 1.8933 (2.1128)  loss_ce_2: 1.9115 (2.1082)  loss_ce_3: 1.9306 (2.1161)  loss_ce_4: 1.9006 (2.0923)  loss_giou: 1.6168 (1.8101)  loss_giou_0: 1.6464 (1.7866)  loss_giou_1: 1.6716 (1.7865)  loss_giou_2: 1.5964 (1.7843)  loss_giou_3: 1.6176 (1.7901)  loss_giou_4: 1.6697 (1.7956)  cardinality_error_unscaled: 5.6250 (7.1155)  cardinality_error_0_unscaled: 5.6250 (7.6002)  cardinality_error_1_unscaled: 5.6250 (7.4329)  cardinality_error_2_unscaled: 5.6250 (7.2430)  cardinality_error_3_unscaled: 5.6250 (7.1138)  cardinality_error_4_unscaled: 5.6250 (7.1279)  class_error_unscaled: 100.0000 (99.6992)  loss_bbox_unscaled: 0.2783 (0.3795)  loss_bbox_0_unscaled: 0.2889 (0.3717)  loss_bbox_1_unscaled: 0.2794 (0.3680)  loss_bbox_2_unscaled: 0.2904 (0.3739)  loss_bbox_3_unscaled: 0.2857 (0.3715)  loss_bbox_4_unscaled: 0.2842 (0.3734)  loss_ce_unscaled: 1.9315 (2.0846)  loss_ce_0_unscaled: 1.9189 (2.1740)  loss_ce_1_unscaled: 1.8933 (2.1128)  loss_ce_2_unscaled: 1.9115 (2.1082)  loss_ce_3_unscaled: 1.9306 (2.1161)  loss_ce_4_unscaled: 1.9006 (2.0923)  loss_giou_unscaled: 0.8084 (0.9050)  loss_giou_0_unscaled: 0.8232 (0.8933)  loss_giou_1_unscaled: 0.8358 (0.8932)  loss_giou_2_unscaled: 0.7982 (0.8921)  loss_giou_3_unscaled: 0.8088 (0.8951)  loss_giou_4_unscaled: 0.8348 (0.8978)  time: 0.4858  data: 0.0086  max mem: 7067
Epoch: [0]  [ 160/7393]  eta: 1:00:07  lr: 0.000100  class_error: 100.00  loss: 31.4487 (34.3920)  loss_bbox: 1.4179 (1.8724)  loss_bbox_0: 1.4668 (1.8379)  loss_bbox_1: 1.4886 (1.8198)  loss_bbox_2: 1.4998 (1.8490)  loss_bbox_3: 1.4284 (1.8362)  loss_bbox_4: 1.4241 (1.8439)  loss_ce: 1.8440 (2.0698)  loss_ce_0: 1.8889 (2.1562)  loss_ce_1: 1.8715 (2.0976)  loss_ce_2: 1.8860 (2.0941)  loss_ce_3: 1.9187 (2.1013)  loss_ce_4: 1.8587 (2.0777)  loss_giou: 1.7220 (1.8075)  loss_giou_0: 1.7850 (1.7869)  loss_giou_1: 1.7717 (1.7842)  loss_giou_2: 1.7050 (1.7798)  loss_giou_3: 1.7276 (1.7860)  loss_giou_4: 1.7129 (1.7917)  cardinality_error_unscaled: 6.1250 (7.0509)  cardinality_error_0_unscaled: 6.1250 (7.5082)  cardinality_error_1_unscaled: 6.0000 (7.3482)  cardinality_error_2_unscaled: 6.1250 (7.1731)  cardinality_error_3_unscaled: 6.1250 (7.0489)  cardinality_error_4_unscaled: 6.1250 (7.0652)  class_error_unscaled: 100.0000 (99.7179)  loss_bbox_unscaled: 0.2836 (0.3745)  loss_bbox_0_unscaled: 0.2934 (0.3676)  loss_bbox_1_unscaled: 0.2977 (0.3640)  loss_bbox_2_unscaled: 0.3000 (0.3698)  loss_bbox_3_unscaled: 0.2857 (0.3672)  loss_bbox_4_unscaled: 0.2848 (0.3688)  loss_ce_unscaled: 1.8440 (2.0698)  loss_ce_0_unscaled: 1.8889 (2.1562)  loss_ce_1_unscaled: 1.8715 (2.0976)  loss_ce_2_unscaled: 1.8860 (2.0941)  loss_ce_3_unscaled: 1.9187 (2.1013)  loss_ce_4_unscaled: 1.8587 (2.0777)  loss_giou_unscaled: 0.8610 (0.9037)  loss_giou_0_unscaled: 0.8925 (0.8935)  loss_giou_1_unscaled: 0.8858 (0.8921)  loss_giou_2_unscaled: 0.8525 (0.8899)  loss_giou_3_unscaled: 0.8638 (0.8930)  loss_giou_4_unscaled: 0.8564 (0.8958)  time: 0.4885  data: 0.0085  max mem: 7067
Epoch: [0]  [ 170/7393]  eta: 0:59:56  lr: 0.000100  class_error: 100.00  loss: 30.3285 (34.2133)  loss_bbox: 1.4183 (1.8459)  loss_bbox_0: 1.4193 (1.8096)  loss_bbox_1: 1.4114 (1.7919)  loss_bbox_2: 1.4472 (1.8210)  loss_bbox_3: 1.4127 (1.8114)  loss_bbox_4: 1.4476 (1.8200)  loss_ce: 1.8863 (2.0699)  loss_ce_0: 1.9283 (2.1539)  loss_ce_1: 1.9204 (2.0978)  loss_ce_2: 1.9210 (2.0947)  loss_ce_3: 1.9303 (2.1005)  loss_ce_4: 1.8918 (2.0775)  loss_giou: 1.7104 (1.8045)  loss_giou_0: 1.7502 (1.7836)  loss_giou_1: 1.7401 (1.7802)  loss_giou_2: 1.6975 (1.7766)  loss_giou_3: 1.7202 (1.7831)  loss_giou_4: 1.7285 (1.7912)  cardinality_error_unscaled: 6.0625 (7.0256)  cardinality_error_0_unscaled: 6.0625 (7.4605)  cardinality_error_1_unscaled: 6.0000 (7.3099)  cardinality_error_2_unscaled: 6.0625 (7.1451)  cardinality_error_3_unscaled: 6.0625 (7.0270)  cardinality_error_4_unscaled: 6.0625 (7.0369)  class_error_unscaled: 100.0000 (99.7344)  loss_bbox_unscaled: 0.2837 (0.3692)  loss_bbox_0_unscaled: 0.2839 (0.3619)  loss_bbox_1_unscaled: 0.2823 (0.3584)  loss_bbox_2_unscaled: 0.2894 (0.3642)  loss_bbox_3_unscaled: 0.2825 (0.3623)  loss_bbox_4_unscaled: 0.2895 (0.3640)  loss_ce_unscaled: 1.8863 (2.0699)  loss_ce_0_unscaled: 1.9283 (2.1539)  loss_ce_1_unscaled: 1.9204 (2.0978)  loss_ce_2_unscaled: 1.9210 (2.0947)  loss_ce_3_unscaled: 1.9303 (2.1005)  loss_ce_4_unscaled: 1.8918 (2.0775)  loss_giou_unscaled: 0.8552 (0.9022)  loss_giou_0_unscaled: 0.8751 (0.8918)  loss_giou_1_unscaled: 0.8700 (0.8901)  loss_giou_2_unscaled: 0.8487 (0.8883)  loss_giou_3_unscaled: 0.8601 (0.8915)  loss_giou_4_unscaled: 0.8642 (0.8956)  time: 0.4869  data: 0.0090  max mem: 7067
Epoch: [0]  [ 180/7393]  eta: 0:59:43  lr: 0.000100  class_error: 100.00  loss: 30.1177 (34.0155)  loss_bbox: 1.4428 (1.8255)  loss_bbox_0: 1.3930 (1.7881)  loss_bbox_1: 1.3746 (1.7706)  loss_bbox_2: 1.3923 (1.7985)  loss_bbox_3: 1.4157 (1.7927)  loss_bbox_4: 1.3978 (1.7978)  loss_ce: 1.9259 (2.0648)  loss_ce_0: 1.9290 (2.1451)  loss_ce_1: 1.9564 (2.0925)  loss_ce_2: 1.9595 (2.0900)  loss_ce_3: 2.0036 (2.0989)  loss_ce_4: 1.9160 (2.0726)  loss_giou: 1.6828 (1.7993)  loss_giou_0: 1.6684 (1.7764)  loss_giou_1: 1.6510 (1.7741)  loss_giou_2: 1.6756 (1.7703)  loss_giou_3: 1.6394 (1.7743)  loss_giou_4: 1.6655 (1.7842)  cardinality_error_unscaled: 5.8125 (6.9952)  cardinality_error_0_unscaled: 5.8125 (7.4092)  cardinality_error_1_unscaled: 5.8125 (7.2666)  cardinality_error_2_unscaled: 5.8125 (7.1112)  cardinality_error_3_unscaled: 5.8125 (6.9962)  cardinality_error_4_unscaled: 5.8125 (7.0079)  class_error_unscaled: 100.0000 (99.7437)  loss_bbox_unscaled: 0.2886 (0.3651)  loss_bbox_0_unscaled: 0.2786 (0.3576)  loss_bbox_1_unscaled: 0.2749 (0.3541)  loss_bbox_2_unscaled: 0.2785 (0.3597)  loss_bbox_3_unscaled: 0.2831 (0.3585)  loss_bbox_4_unscaled: 0.2796 (0.3596)  loss_ce_unscaled: 1.9259 (2.0648)  loss_ce_0_unscaled: 1.9290 (2.1451)  loss_ce_1_unscaled: 1.9564 (2.0925)  loss_ce_2_unscaled: 1.9595 (2.0900)  loss_ce_3_unscaled: 2.0036 (2.0989)  loss_ce_4_unscaled: 1.9160 (2.0726)  loss_giou_unscaled: 0.8414 (0.8996)  loss_giou_0_unscaled: 0.8342 (0.8882)  loss_giou_1_unscaled: 0.8255 (0.8870)  loss_giou_2_unscaled: 0.8378 (0.8852)  loss_giou_3_unscaled: 0.8197 (0.8872)  loss_giou_4_unscaled: 0.8328 (0.8921)  time: 0.4809  data: 0.0089  max mem: 7067
Epoch: [0]  [ 190/7393]  eta: 0:59:34  lr: 0.000100  class_error: 100.00  loss: 30.0076 (33.8008)  loss_bbox: 1.4556 (1.8062)  loss_bbox_0: 1.3772 (1.7651)  loss_bbox_1: 1.3746 (1.7504)  loss_bbox_2: 1.3881 (1.7787)  loss_bbox_3: 1.4293 (1.7730)  loss_bbox_4: 1.3978 (1.7766)  loss_ce: 1.8913 (2.0577)  loss_ce_0: 1.9140 (2.1338)  loss_ce_1: 1.9173 (2.0844)  loss_ce_2: 1.9242 (2.0816)  loss_ce_3: 1.9818 (2.0905)  loss_ce_4: 1.9021 (2.0642)  loss_giou: 1.6588 (1.7919)  loss_giou_0: 1.6330 (1.7697)  loss_giou_1: 1.6369 (1.7674)  loss_giou_2: 1.6369 (1.7643)  loss_giou_3: 1.6194 (1.7679)  loss_giou_4: 1.6399 (1.7771)  cardinality_error_unscaled: 5.8125 (6.9604)  cardinality_error_0_unscaled: 5.8125 (7.3652)  cardinality_error_1_unscaled: 5.8125 (7.2134)  cardinality_error_2_unscaled: 5.8125 (7.0828)  cardinality_error_3_unscaled: 5.7500 (6.9712)  cardinality_error_4_unscaled: 5.8125 (6.9846)  class_error_unscaled: 100.0000 (99.7278)  loss_bbox_unscaled: 0.2911 (0.3612)  loss_bbox_0_unscaled: 0.2754 (0.3530)  loss_bbox_1_unscaled: 0.2749 (0.3501)  loss_bbox_2_unscaled: 0.2776 (0.3557)  loss_bbox_3_unscaled: 0.2859 (0.3546)  loss_bbox_4_unscaled: 0.2796 (0.3553)  loss_ce_unscaled: 1.8913 (2.0577)  loss_ce_0_unscaled: 1.9140 (2.1338)  loss_ce_1_unscaled: 1.9173 (2.0844)  loss_ce_2_unscaled: 1.9242 (2.0816)  loss_ce_3_unscaled: 1.9818 (2.0905)  loss_ce_4_unscaled: 1.9021 (2.0642)  loss_giou_unscaled: 0.8294 (0.8960)  loss_giou_0_unscaled: 0.8165 (0.8849)  loss_giou_1_unscaled: 0.8185 (0.8837)  loss_giou_2_unscaled: 0.8184 (0.8821)  loss_giou_3_unscaled: 0.8097 (0.8840)  loss_giou_4_unscaled: 0.8199 (0.8885)  time: 0.4817  data: 0.0085  max mem: 7067
Epoch: [0]  [ 200/7393]  eta: 0:59:29  lr: 0.000100  class_error: 100.00  loss: 30.8417 (33.6732)  loss_bbox: 1.4284 (1.7864)  loss_bbox_0: 1.4247 (1.7513)  loss_bbox_1: 1.3584 (1.7357)  loss_bbox_2: 1.3967 (1.7639)  loss_bbox_3: 1.3814 (1.7545)  loss_bbox_4: 1.4030 (1.7594)  loss_ce: 1.9454 (2.0547)  loss_ce_0: 1.9688 (2.1306)  loss_ce_1: 1.9597 (2.0833)  loss_ce_2: 1.9653 (2.0785)  loss_ce_3: 1.9753 (2.0860)  loss_ce_4: 1.9386 (2.0616)  loss_giou: 1.6936 (1.7887)  loss_giou_0: 1.6897 (1.7703)  loss_giou_1: 1.6643 (1.7657)  loss_giou_2: 1.6488 (1.7625)  loss_giou_3: 1.6783 (1.7672)  loss_giou_4: 1.6681 (1.7727)  cardinality_error_unscaled: 6.8750 (6.9490)  cardinality_error_0_unscaled: 6.8750 (7.3405)  cardinality_error_1_unscaled: 6.5000 (7.1744)  cardinality_error_2_unscaled: 6.8750 (7.0697)  cardinality_error_3_unscaled: 6.8125 (6.9614)  cardinality_error_4_unscaled: 6.8750 (6.9599)  class_error_unscaled: 100.0000 (99.7414)  loss_bbox_unscaled: 0.2857 (0.3573)  loss_bbox_0_unscaled: 0.2849 (0.3503)  loss_bbox_1_unscaled: 0.2717 (0.3471)  loss_bbox_2_unscaled: 0.2793 (0.3528)  loss_bbox_3_unscaled: 0.2763 (0.3509)  loss_bbox_4_unscaled: 0.2806 (0.3519)  loss_ce_unscaled: 1.9454 (2.0547)  loss_ce_0_unscaled: 1.9688 (2.1306)  loss_ce_1_unscaled: 1.9597 (2.0833)  loss_ce_2_unscaled: 1.9653 (2.0785)  loss_ce_3_unscaled: 1.9753 (2.0860)  loss_ce_4_unscaled: 1.9386 (2.0616)  loss_giou_unscaled: 0.8468 (0.8944)  loss_giou_0_unscaled: 0.8449 (0.8852)  loss_giou_1_unscaled: 0.8322 (0.8828)  loss_giou_2_unscaled: 0.8244 (0.8813)  loss_giou_3_unscaled: 0.8391 (0.8836)  loss_giou_4_unscaled: 0.8340 (0.8863)  time: 0.4912  data: 0.0088  max mem: 7067
Epoch: [0]  [ 210/7393]  eta: 0:59:19  lr: 0.000100  class_error: 100.00  loss: 30.9279 (33.5228)  loss_bbox: 1.4284 (1.7695)  loss_bbox_0: 1.3630 (1.7311)  loss_bbox_1: 1.3855 (1.7193)  loss_bbox_2: 1.4696 (1.7496)  loss_bbox_3: 1.3929 (1.7395)  loss_bbox_4: 1.4123 (1.7439)  loss_ce: 1.9781 (2.0480)  loss_ce_0: 2.0093 (2.1236)  loss_ce_1: 2.0127 (2.0780)  loss_ce_2: 2.0082 (2.0737)  loss_ce_3: 2.0080 (2.0797)  loss_ce_4: 1.9874 (2.0558)  loss_giou: 1.7359 (1.7880)  loss_giou_0: 1.7498 (1.7663)  loss_giou_1: 1.7164 (1.7628)  loss_giou_2: 1.6949 (1.7597)  loss_giou_3: 1.7301 (1.7649)  loss_giou_4: 1.6913 (1.7696)  cardinality_error_unscaled: 6.9375 (6.9378)  cardinality_error_0_unscaled: 6.9375 (7.3110)  cardinality_error_1_unscaled: 6.8750 (7.1523)  cardinality_error_2_unscaled: 6.9375 (7.0530)  cardinality_error_3_unscaled: 6.9375 (6.9499)  cardinality_error_4_unscaled: 6.9375 (6.9449)  class_error_unscaled: 100.0000 (99.7536)  loss_bbox_unscaled: 0.2857 (0.3539)  loss_bbox_0_unscaled: 0.2726 (0.3462)  loss_bbox_1_unscaled: 0.2771 (0.3439)  loss_bbox_2_unscaled: 0.2939 (0.3499)  loss_bbox_3_unscaled: 0.2786 (0.3479)  loss_bbox_4_unscaled: 0.2825 (0.3488)  loss_ce_unscaled: 1.9781 (2.0480)  loss_ce_0_unscaled: 2.0093 (2.1236)  loss_ce_1_unscaled: 2.0127 (2.0780)  loss_ce_2_unscaled: 2.0082 (2.0737)  loss_ce_3_unscaled: 2.0080 (2.0797)  loss_ce_4_unscaled: 1.9874 (2.0558)  loss_giou_unscaled: 0.8679 (0.8940)  loss_giou_0_unscaled: 0.8749 (0.8831)  loss_giou_1_unscaled: 0.8582 (0.8814)  loss_giou_2_unscaled: 0.8475 (0.8798)  loss_giou_3_unscaled: 0.8651 (0.8824)  loss_giou_4_unscaled: 0.8456 (0.8848)  time: 0.4893  data: 0.0088  max mem: 7067
Epoch: [0]  [ 220/7393]  eta: 0:59:11  lr: 0.000100  class_error: 100.00  loss: 30.7641 (33.4134)  loss_bbox: 1.4808 (1.7572)  loss_bbox_0: 1.3330 (1.7155)  loss_bbox_1: 1.3703 (1.7049)  loss_bbox_2: 1.4477 (1.7378)  loss_bbox_3: 1.4422 (1.7280)  loss_bbox_4: 1.4652 (1.7334)  loss_ce: 1.8782 (2.0443)  loss_ce_0: 1.9348 (2.1152)  loss_ce_1: 1.9243 (2.0710)  loss_ce_2: 1.9136 (2.0672)  loss_ce_3: 1.9298 (2.0730)  loss_ce_4: 1.9154 (2.0508)  loss_giou: 1.7861 (1.7900)  loss_giou_0: 1.7378 (1.7669)  loss_giou_1: 1.7618 (1.7634)  loss_giou_2: 1.7448 (1.7598)  loss_giou_3: 1.7337 (1.7650)  loss_giou_4: 1.7788 (1.7702)  cardinality_error_unscaled: 6.7500 (6.9140)  cardinality_error_0_unscaled: 6.7500 (7.2794)  cardinality_error_1_unscaled: 6.7500 (7.1273)  cardinality_error_2_unscaled: 6.7500 (7.0331)  cardinality_error_3_unscaled: 6.7500 (6.9338)  cardinality_error_4_unscaled: 6.7500 (6.9262)  class_error_unscaled: 100.0000 (99.7591)  loss_bbox_unscaled: 0.2962 (0.3514)  loss_bbox_0_unscaled: 0.2666 (0.3431)  loss_bbox_1_unscaled: 0.2741 (0.3410)  loss_bbox_2_unscaled: 0.2895 (0.3476)  loss_bbox_3_unscaled: 0.2884 (0.3456)  loss_bbox_4_unscaled: 0.2930 (0.3467)  loss_ce_unscaled: 1.8782 (2.0443)  loss_ce_0_unscaled: 1.9348 (2.1152)  loss_ce_1_unscaled: 1.9243 (2.0710)  loss_ce_2_unscaled: 1.9136 (2.0672)  loss_ce_3_unscaled: 1.9298 (2.0730)  loss_ce_4_unscaled: 1.9154 (2.0508)  loss_giou_unscaled: 0.8930 (0.8950)  loss_giou_0_unscaled: 0.8689 (0.8834)  loss_giou_1_unscaled: 0.8809 (0.8817)  loss_giou_2_unscaled: 0.8724 (0.8799)  loss_giou_3_unscaled: 0.8668 (0.8825)  loss_giou_4_unscaled: 0.8894 (0.8851)  time: 0.4849  data: 0.0085  max mem: 7067
Epoch: [0]  [ 230/7393]  eta: 0:59:03  lr: 0.000100  class_error: 100.00  loss: 31.1909 (33.3535)  loss_bbox: 1.4568 (1.7427)  loss_bbox_0: 1.3330 (1.7005)  loss_bbox_1: 1.3924 (1.6937)  loss_bbox_2: 1.4087 (1.7240)  loss_bbox_3: 1.4031 (1.7128)  loss_bbox_4: 1.4263 (1.7185)  loss_ce: 2.0615 (2.0494)  loss_ce_0: 1.9941 (2.1158)  loss_ce_1: 2.0017 (2.0749)  loss_ce_2: 2.0135 (2.0716)  loss_ce_3: 1.9809 (2.0752)  loss_ce_4: 2.0215 (2.0546)  loss_giou: 1.7861 (1.7899)  loss_giou_0: 1.7653 (1.7661)  loss_giou_1: 1.7879 (1.7653)  loss_giou_2: 1.7983 (1.7606)  loss_giou_3: 1.7766 (1.7658)  loss_giou_4: 1.7902 (1.7720)  cardinality_error_unscaled: 6.7500 (6.9164)  cardinality_error_0_unscaled: 6.7500 (7.2700)  cardinality_error_1_unscaled: 6.7500 (7.1245)  cardinality_error_2_unscaled: 6.7500 (7.0333)  cardinality_error_3_unscaled: 6.7500 (6.9370)  cardinality_error_4_unscaled: 6.6875 (6.9315)  class_error_unscaled: 100.0000 (99.7695)  loss_bbox_unscaled: 0.2914 (0.3485)  loss_bbox_0_unscaled: 0.2666 (0.3401)  loss_bbox_1_unscaled: 0.2785 (0.3387)  loss_bbox_2_unscaled: 0.2817 (0.3448)  loss_bbox_3_unscaled: 0.2806 (0.3426)  loss_bbox_4_unscaled: 0.2853 (0.3437)  loss_ce_unscaled: 2.0615 (2.0494)  loss_ce_0_unscaled: 1.9941 (2.1158)  loss_ce_1_unscaled: 2.0017 (2.0749)  loss_ce_2_unscaled: 2.0135 (2.0716)  loss_ce_3_unscaled: 1.9809 (2.0752)  loss_ce_4_unscaled: 2.0215 (2.0546)  loss_giou_unscaled: 0.8930 (0.8949)  loss_giou_0_unscaled: 0.8827 (0.8831)  loss_giou_1_unscaled: 0.8939 (0.8827)  loss_giou_2_unscaled: 0.8992 (0.8803)  loss_giou_3_unscaled: 0.8883 (0.8829)  loss_giou_4_unscaled: 0.8951 (0.8860)  time: 0.4862  data: 0.0085  max mem: 7067
Epoch: [0]  [ 240/7393]  eta: 0:58:57  lr: 0.000100  class_error: 100.00  loss: 31.8562 (33.2722)  loss_bbox: 1.4341 (1.7314)  loss_bbox_0: 1.3873 (1.6882)  loss_bbox_1: 1.3924 (1.6795)  loss_bbox_2: 1.4087 (1.7124)  loss_bbox_3: 1.4020 (1.7007)  loss_bbox_4: 1.3950 (1.7065)  loss_ce: 2.1259 (2.0503)  loss_ce_0: 2.0846 (2.1142)  loss_ce_1: 2.1082 (2.0751)  loss_ce_2: 2.1092 (2.0725)  loss_ce_3: 2.0870 (2.0744)  loss_ce_4: 2.1185 (2.0559)  loss_giou: 1.7733 (1.7894)  loss_giou_0: 1.7093 (1.7641)  loss_giou_1: 1.7157 (1.7637)  loss_giou_2: 1.6738 (1.7581)  loss_giou_3: 1.7120 (1.7647)  loss_giou_4: 1.7390 (1.7710)  cardinality_error_unscaled: 6.9375 (6.9251)  cardinality_error_0_unscaled: 6.9375 (7.2645)  cardinality_error_1_unscaled: 6.7500 (7.1237)  cardinality_error_2_unscaled: 6.8750 (7.0360)  cardinality_error_3_unscaled: 6.8125 (6.9411)  cardinality_error_4_unscaled: 6.9375 (6.9396)  class_error_unscaled: 100.0000 (99.7791)  loss_bbox_unscaled: 0.2868 (0.3463)  loss_bbox_0_unscaled: 0.2775 (0.3376)  loss_bbox_1_unscaled: 0.2785 (0.3359)  loss_bbox_2_unscaled: 0.2817 (0.3425)  loss_bbox_3_unscaled: 0.2804 (0.3401)  loss_bbox_4_unscaled: 0.2790 (0.3413)  loss_ce_unscaled: 2.1259 (2.0503)  loss_ce_0_unscaled: 2.0846 (2.1142)  loss_ce_1_unscaled: 2.1082 (2.0751)  loss_ce_2_unscaled: 2.1092 (2.0725)  loss_ce_3_unscaled: 2.0870 (2.0744)  loss_ce_4_unscaled: 2.1185 (2.0559)  loss_giou_unscaled: 0.8867 (0.8947)  loss_giou_0_unscaled: 0.8547 (0.8821)  loss_giou_1_unscaled: 0.8578 (0.8818)  loss_giou_2_unscaled: 0.8369 (0.8790)  loss_giou_3_unscaled: 0.8560 (0.8824)  loss_giou_4_unscaled: 0.8695 (0.8855)  time: 0.4871  data: 0.0088  max mem: 7067
